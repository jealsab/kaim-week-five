{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KjuSYzBSteR0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xpBXtFjAttZM"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "model_name = \"/content/fine-tuned-ner-model\"\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name, output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CXge3VKstorO"
      },
      "outputs": [],
      "source": [
        "def get_attention_weights(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "    attentions = outputs.attentions[-1].squeeze(0)  # Get the attention weights from the last layer\n",
        "    return inputs, attentions\n",
        "# Sample text for interpretation\n",
        "sample_text = \"የገና በዓልን ምክንያት በማድረግ የባሕርዳር ዩኒቨርሲቲ ተማሪዎች ከዝግባ ሕጻናትና አረጋዊያን መርጃ  በጎ አድራጎት ድርጅት በመረዳት ላይ ለሚገኙ ወገኖች የምሳ ግብዣ በማድረግ በዓልን አሳልፈዋል።\"\n",
        "inputs, attentions = get_attention_weights(model, tokenizer, sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cKcWhHMvc8C",
        "outputId": "0153173b-a4e9-4ab9-945b-d5981500696c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention Weights (Descending Order):\n",
            "Token: [SEP], Weight: 0.0618\n",
            "Token: [UNK], Weight: 0.0538\n",
            "Token: [CLS], Weight: 0.0513\n",
            "Token: [UNK], Weight: 0.0488\n",
            "Token: [UNK], Weight: 0.0481\n",
            "Token: [UNK], Weight: 0.0468\n",
            "Token: [UNK], Weight: 0.0450\n",
            "Token: [UNK], Weight: 0.0428\n",
            "Token: [UNK], Weight: 0.0415\n",
            "Token: [UNK], Weight: 0.0403\n",
            "Token: [UNK], Weight: 0.0393\n",
            "Token: [UNK], Weight: 0.0374\n",
            "Token: [UNK], Weight: 0.0362\n",
            "Token: [UNK], Weight: 0.0351\n",
            "Token: [UNK], Weight: 0.0342\n",
            "Token: [UNK], Weight: 0.0334\n",
            "Token: [UNK], Weight: 0.0329\n",
            "Token: [UNK], Weight: 0.0324\n",
            "Token: [UNK], Weight: 0.0324\n",
            "Token: [UNK], Weight: 0.0323\n",
            "Token: [UNK], Weight: 0.0317\n",
            "Token: [UNK], Weight: 0.0302\n",
            "Token: [UNK], Weight: 0.0292\n",
            "Token: [UNK], Weight: 0.0280\n",
            "Token: [UNK], Weight: 0.0277\n",
            "Token: [UNK], Weight: 0.0273\n"
          ]
        }
      ],
      "source": [
        "# Function to print attention weights in descending order\n",
        "def print_attention_weights(attentions, inputs, tokenizer):\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
        "    attention_weights = attentions.mean(dim=0).detach().cpu().numpy()  # Average over heads\n",
        "    token_attention_weights = []\n",
        "    for i, token in enumerate(tokens):\n",
        "        avg_attention = attention_weights[:, i].mean()  # Average attention for each token specifically\n",
        "        token_attention_weights.append((token, avg_attention))\n",
        "    # Sort by attention weight in descending order\n",
        "    sorted_token_attention_weights = sorted(token_attention_weights, key=lambda item: item[1], reverse=True)\n",
        "    print(\"Attention Weights (Descending Order):\")\n",
        "    for token, weight in sorted_token_attention_weights:\n",
        "        print(f\"Token: {token}, Weight: {weight:.4f}\")\n",
        "# Print the attention weights\n",
        "print_attention_weights(attentions, inputs, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
